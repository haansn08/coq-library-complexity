\chapter{Conclusion}\label{chap:conclusion}
In this final chapter we give an overview on related work and comment on potential future work, both from the perspective of formalisation and mechanisation.

\todo{somewhere relate to other proofs of Cook's Theorem}
\section{Related Work}
There are a number of different proofs of the Cook-Levin-Theorem available. In this thesis, we have formalised the tableau-style proof by Sipser (Chapter 7.4 in~\cite{Sipser:TheoryofComputation}), which is in the spirit of Cook's original construction~\cite{cook_theorem}. However, Cook considers the problem of determining tautologihood of DNFs, which he does by first building a CNF and then negating it. The construction is also limited to Turing machines with one-sided infinite tapes.
A class of more modern proofs uses circuits. First of all, a family of circuits is designed which encodes the computation of a Turing machine for different input lengths. Then, it is shown how a circuit can be encoded as a Boolean formula. This proof is available in different flavours, for example the construction of the circuit family can proceed in a tableau-like style (e.g.\ in Chapter 9.3 of~\cite{Sipser:TheoryofComputation}) or by first restricting the Turing machine to be \emph{oblivious}, such that it shows the same sequence of head movements on every input of a certain length~\cite{BlÃ¤ser:TISkript}. 
Although the circuit-based proofs might appear more elegant on paper and their ideas can be used for other proofs in circuit complexity, their mechanisation seems to be a lot more tedious as one would first have to formalise the standard notions of complexity and constructibility of circuits. 

As mentioned in the introduction, there is an existing mechanisation of the translation of Turing machines to Boolean formulas in the theorem prover ACL2~\cite{gamboa:cook}. They also use a tableau-style construction. However, there are a number of key differences to our mechanisation. 
First of all, they restrict their proof to Turing machines with one-sided infinite tapes. By this restriction, they also circumvent the problem of non-uniqueness of representation of a tape which we explained in Chapter~\ref{chap:informaloverview} and addressed by using a moving-tape semantics instead of the standard moving-head semantics. Thus, their proof can use moving-head semantics just fine. 
Moreover, they use nondeterministic Turing machines and have a fixed input, which we did not choose to do because the existing Turing machine framework~\cite{ForsterEtAl:2019:VerifiedTMs} does only include deterministic Turing machines and our definition of NP also uses the verifier characterisation. Their proof ends at the problem which we have dubbed \fsat{}, omitting the additional step to \sat{}. 
From a high-level perspective, the most striking difference is that they do a \emph{direct} reduction instead of factorising the proof as we did. This is quite interesting, as we originally thought it would not be feasible to do a direct reduction and deal with the encoding of transitions, the encoding of arbitrary finite alphabets, and the handling of variables in the resulting formula all at once. Motivated by this thought process, we developed our factorisation and only after finishing the mechanisation became aware of the existing implementation. 
However, we think that this factorisation leads to the proof to be much more elegant and understandable. While it seems hard to get more than a high-level intuitive grasp of the proof~\cite{gamboa:cook}, it is our belief that our proof is quite satisfactory even on paper.
Finally, the authors remark that ACL2 in its then-current state is not suitable for a running-time analysis. Due to the lack of alternatives, they do their analysis by defining a second version of their translation functions that count the number of steps the translation takes. It is, however, not clear at all that the resulting cost model is reasonable in the sense explained in Section~\ref{sec:time_and_space}.
Due to this lack of connection to a real machine model, we are of the opinion that their result does not fully include the Cook-Levin-Theorem, which clearly requires to derive the \NP{}-hardness of a (natural) problem.
The full running-time analysis with respect to a reasonable computational model we see as the major advantage of our proof.

There have been other attempts at formalising complexity-theoretic results.\todo{yes}

\section{Future Work}
In this thesis, we have formally verified a reduction from Turing machines to \SAT{}. However, as we are working in the setting of complexity theory in L, we would eventually like to obtain a mechanised reduction from L to \SAT{}. 
The missing reduction from L to Turing machines is certainly challenging as it requires the implementation of a heap-based evaluation strategy for the lambda calculus using Turing machines.\footnote{Note that the more complicated interleaving strategy of~\cite{ForsterKunzeRoth:2019:wcbv-Reasonable} is not necessary as we do not care about the space overhead.} The framework for the verified programming of Turing machines~\cite{ForsterEtAl:2019:VerifiedTMs}, whose formalisation of Turing machines is used in this thesis, will make this much easier as it allows to do relatively high-level programming with control-flow primitives and ways to encode inductive datatypes on individual tapes. 
Then, a compiler from multi-tape to single-tape Turing machines is necessary, which has already been verified in~\cite{ForsterEtAl:2019:VerifiedTMs}. Note that one can fix a constant Turing machine which takes the L-term to simulate as an input, so that the running time of the multi-tape to single-tape reduction need not be analysed. 
Currently, this reduction is work-in-progress.

Another relatively isolated line of work is to define a version of L including nondeterminism and to prove that the resulting definition of \NP{} agrees with the verifier characterisation we use here. This might also open the door to formalise other results of complexity theory which rely on nondeterminism. 

For the proofs in this thesis, we were mostly only concerned with the time usage of a reduction and not its space usage (we only made sure that it does not exhibit size explosion). It would be interesting to formalise space classes like \textsf{LogSpace} and \textsf{PSpace} and prove results like Savitch's Theorem which relates nondeterministic and deterministic space. For a mechanisation to be possible, first the extraction framework would need to be expanded to also derive recurrences for the space usage of a term.

On a more general note, the expansion of the reasonability result of~\cite{ForsterKunzeRoth:2019:wcbv-Reasonable} beyond decision problems would be interesting in order to facilitate the mechanisation of results outside of the mere complexity theory of decision problems. It seems like one might need to investigate the matter of size explosion in more detail in order for this to work, maybe resulting in general conditions to rule out size explosion or a direct scheme to compress size-exploding terms. 

Finally, on the more technical side there is the issue of binary encodings. Throughout this thesis we used a unary encoding for numbers. For the problems we considered this is reasonable (or even necessary, as with the encoding of the number of steps in \gennp{}), but many number-theoretic problems are not \NP{}-hard anymore if one uses a unary encoding instead of a binary encoding\footnote{or any other $c$-nary encoding for $c> 1$}. An example for this is the \textsf{SubsetSum} problem, asking whether there is a subset $A \subseteq$ of a given set of numbers $B$ such that $A$ sums exactly to a given number $c$. 
However, almost all Coq standard library functions are defined with respect to the unary Peano numbers $\nat$. 
Having to redefine them for binary numbers would be rather unpleasant, especially as writing recursive functions on binary numbers is more difficult\footnote{The Peano recursor for binary numbers seems like the best way, but clutters up the function's definition.}. 
A translation can usually be done mechanically, which is why we feel like it might be feasible to implement an automatic translation using the MetaCoq project~\cite{metacoq_web} for meta progamming in Coq. This translation could also automatically prove agreement with the original unary definition. 

